# LLM Client

ä¸€ä¸ªåŸºäº httpx çš„ç®€æ´ã€å¼ºç±»å‹çš„ OpenAI å…¼å®¹ API å®¢æˆ·ç«¯ï¼Œæ”¯æŒæ–‡æœ¬å’Œå¤šæ¨¡æ€ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰èŠå¤©ã€‚

## ç‰¹æ€§

- ğŸš€ **ç®€æ´çš„APIè®¾è®¡**ï¼šç®€å•çš„è¾“å…¥è¾“å‡ºæ¥å£ï¼Œæ˜“äºä½¿ç”¨
- ğŸ–¼ï¸ **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ”¯æŒæ–‡æœ¬å’Œå›¾ç‰‡è¾“å…¥ï¼Œå›¾ç‰‡å¯ä»¥ä»è·¯å¾„è¯»å–æˆ–ç›´æ¥ä¼ å…¥bytes
- ğŸ”‘ **çµæ´»é…ç½®**ï¼šé€šè¿‡å‚æ•°æˆ–ç¯å¢ƒå˜é‡é…ç½®APIå¯†é’¥å’ŒåŸºç¡€URL
- ğŸ“¡ **æµå¼å’Œéæµå¼**ï¼šæ”¯æŒå®æ—¶æµå¼å“åº”å’Œæ ‡å‡†å“åº”
- ğŸ“Š **è¯¦ç»†ç»Ÿè®¡**ï¼šè¿”å›è¾“å…¥/è¾“å‡ºtokenç»Ÿè®¡ä¿¡æ¯
- ğŸ¯ **å¼ºç±»å‹**ï¼šä½¿ç”¨Pydanticæ¨¡å‹æä¾›å®Œæ•´çš„IDEè¡¥å…¨å’Œç±»å‹æ£€æŸ¥
- ğŸ”Œ **OpenAIå…¼å®¹**ï¼šæ”¯æŒæ‰€æœ‰OpenAIå…¼å®¹çš„APIæœåŠ¡

## å®‰è£…

```bash
uv add git+https://github.com/Eslzzyl/llm-client
```

## å¿«é€Ÿå¼€å§‹

### 1. è®¾ç½®ç¯å¢ƒå˜é‡

```bash
# Windows PowerShell
$env:OPENAI_API_KEY = "your-api-key"
$env:OPENAI_BASE_URL = "https://api.openai.com/v1"  # å¯é€‰ï¼Œé»˜è®¤ä¸ºOpenAIå®˜æ–¹API

# Linux/Mac
export OPENAI_API_KEY="your-api-key"
export OPENAI_BASE_URL="https://api.openai.com/v1"  # å¯é€‰
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
from llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
with LLMClient() as client:
    # ç®€å•æ–‡æœ¬èŠå¤©
    response = client.simple_chat(
        text="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å–„çš„AIåŠ©æ‰‹"
    )
    
    print(f"å›å¤: {response.content}")
    print(f"Tokenä½¿ç”¨: è¾“å…¥ {response.input_tokens}, è¾“å‡º {response.output_tokens}")
```

### 3. å¤šæ¨¡æ€èŠå¤©ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰

```python
# æ”¯æŒå›¾ç‰‡è·¯å¾„
response = client.simple_chat(
    text="è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
    images=["path/to/image.jpg"]
)

# æ”¯æŒå›¾ç‰‡bytes
with open("image.jpg", "rb") as f:
    image_bytes = f.read()

response = client.simple_chat(
    text="åˆ†æè¿™å¼ å›¾ç‰‡",
    images=[image_bytes]
)
```

### 4. æµå¼å“åº”

```python
# æµå¼èŠå¤©
for chunk in client.simple_chat_stream(
    text="è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªè¯—äºº"
):
    print(chunk.content, end="", flush=True)
```

### 5. å¯¹è¯å†å²

```python
from llm_client import Message

# æ„å»ºå¯¹è¯
messages = [
    Message.user_text("æˆ‘æƒ³å­¦ä¹ Python"),
    Message.assistant("Pythonæ˜¯ä¸€é—¨å¾ˆæ£’çš„ç¼–ç¨‹è¯­è¨€..."),
    Message.user_text("é‚£æˆ‘åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ")
]

response = client.chat(
    messages=messages,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹å¯¼å¸ˆ"
)
```

## API å‚è€ƒ

### LLMClient

ä¸»è¦çš„å®¢æˆ·ç«¯ç±»ï¼Œæä¾›ä¸OpenAIå…¼å®¹APIçš„æ¥å£ã€‚

```python
client = LLMClient(
    api_key="your-key",          # APIå¯†é’¥ï¼Œå¯é€‰ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    base_url="https://...",      # APIåŸºç¡€URLï¼Œå¯é€‰
    model="gpt-3.5-turbo",      # é»˜è®¤æ¨¡å‹
    timeout=60.0                # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
)
```

#### æ–¹æ³•

- `simple_chat()` - ç®€å•èŠå¤©æ¥å£
- `simple_chat_stream()` - ç®€å•æµå¼èŠå¤©æ¥å£  
- `chat()` - å®Œæ•´èŠå¤©æ¥å£
- `close()` - å…³é—­å®¢æˆ·ç«¯

### æ¶ˆæ¯ç±»å‹

#### Message

è¡¨ç¤ºèŠå¤©æ¶ˆæ¯çš„åŸºç¡€ç±»ï¼š

```python
# åˆ›å»ºå„ç§ç±»å‹çš„æ¶ˆæ¯
Message.user_text("ç”¨æˆ·æ–‡æœ¬")
Message.user_multimodal("æ–‡æœ¬", images=["path.jpg"])
Message.assistant("åŠ©æ‰‹å›å¤")
Message.system("ç³»ç»Ÿæç¤º")
```

#### å›¾ç‰‡å¤„ç†

```python
# ä»è·¯å¾„åˆ›å»ºå›¾ç‰‡
ImageContent.from_path("image.jpg")

# ä»bytesåˆ›å»ºå›¾ç‰‡
ImageContent.from_bytes(image_bytes, "image/jpeg")
```

### å“åº”ç±»å‹

#### ChatCompletion

éæµå¼å“åº”çš„ç»“æœï¼š

```python
response = client.simple_chat("ä½ å¥½")

# è®¿é—®å›å¤å†…å®¹
print(response.content)

# è®¿é—®ç»Ÿè®¡ä¿¡æ¯
print(response.input_tokens)    # è¾“å…¥tokenæ•°
print(response.output_tokens)   # è¾“å‡ºtokenæ•°  
print(response.total_tokens)    # æ€»tokenæ•°

# åŸå§‹å“åº”
print(response.choices[0].message.content)
print(response.usage.prompt_tokens)
```

#### StreamChunk

æµå¼å“åº”çš„æ¯ä¸ªå—ï¼š

```python
for chunk in client.simple_chat_stream("ä½ å¥½"):
    print(chunk.content)        # å¢é‡å†…å®¹
    print(chunk.is_finished)    # æ˜¯å¦å®Œæˆ
    
    if chunk.usage:             # æœ€åä¸€ä¸ªchunkåŒ…å«ç»Ÿè®¡
        print(chunk.usage.total_tokens)
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰å‚æ•°

```python
response = client.chat(
    messages=[Message.user_text("å†™ä¸€ä¸ªæ•…äº‹")],
    temperature=0.8,      # æ§åˆ¶éšæœºæ€§
    max_tokens=1000,      # é™åˆ¶è¾“å‡ºé•¿åº¦
    top_p=0.9,           # nucleus sampling
    frequency_penalty=0.5 # é¢‘ç‡æƒ©ç½š
)
```

### é”™è¯¯å¤„ç†

```python
try:
    response = client.simple_chat("ä½ å¥½")
except httpx.HTTPStatusError as e:
    print(f"HTTPé”™è¯¯: {e.response.status_code}")
except httpx.TimeoutException:
    print("è¯·æ±‚è¶…æ—¶")
except ValueError as e:
    print(f"é…ç½®é”™è¯¯: {e}")
```

### ä¸åŒçš„APIæä¾›å•†

```python
# OpenAI
client = LLMClient(
    base_url="https://api.openai.com/v1",
    model="gpt-4"
)

# å…¶ä»–å…¼å®¹æœåŠ¡
client = LLMClient(
    base_url="https://api.deepseek.com/v1",
    model="deepseek-chat"
)
```

## ä¾èµ–

- [httpx](https://www.python-httpx.org/) - ç°ä»£HTTPå®¢æˆ·ç«¯
- [pydantic](https://pydantic-docs.helpmanual.io/) - æ•°æ®éªŒè¯å’Œç±»å‹æç¤º
